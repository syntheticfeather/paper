好的，我们来深入学习概率论的第四章——**随机变量的数字特征**。这一章的核心思想是：用一些关键的“数字”来概括一个随机变量或一组随机变量的整体统计规律，而不是去处理整个复杂的分布函数。

---

### **第4.1节 数学期望 (Mathematical Expectation)**

数学期望，也称为**均值**，是随机变量取值的“加权平均”，权重就是其对应的概率。

#### **1. 定义**

*   **离散型随机变量**：
    设X的分布律为 $P(X = x_i) = p_i, i=1,2,...$，如果级数 $\sum_{i=1}^{\infty} |x_i| p_i < +\infty$（绝对收敛），则X的数学期望为：
    $E(X) = \sum_{i=1}^{\infty} x_i p_i$

*   **连续型随机变量**：
    设X的概率密度为 $f(x)$，如果积分 $\int_{-\infty}^{+\infty} |x| f(x) dx < +\infty$，则X的数学期望为：
    $E(X) = \int_{-\infty}^{+\infty} x f(x) dx$

> **注**：并非所有随机变量都有数学期望。例如柯西分布就没有数学期望。

#### **2. 随机变量函数的数学期望**

这是非常重要的定理，它允许我们不求Y=g(X)的分布，直接计算其期望。

*   若Y = g(X)，且g(x)连续，则：
    *   X离散时：$E(Y) = E[g(X)] = \sum_{i=1}^{\infty} g(x_i) p_i$
    *   X连续时：$E(Y) = E[g(X)] = \int_{-\infty}^{+\infty} g(x) f(x) dx$

#### **3. 数学期望的性质**

设X, Y是随机变量，c, b是常数。

1.  **常数的期望**：$E(c) = c$
2.  **线性性**：
    *   $E(cX) = cE(X)$
    *   $E(cX + b) = Σ(cX + b)pi = ΣcXpi + Σbpi = cE(X) + b$
    *   $E(X + Y) = E(X) + E(Y)$ （可推广到n个）
3.  **独立变量的乘积**：
    *   若X, Y相互独立，则 $E(XY) = E(X)E(Y)$

> **重要应用**：即使两个随机变量不独立，它们的和的期望仍然等于期望的和。这在实际计算中非常方便。

#### **4. 常见分布的数学期望**
*   $X \sim B(n, p)$：$E(X) = np$
*   $X \sim P(\lambda)$：$E(X) = \lambda$
*   $X \sim N(\mu, \sigma^2)$：$E(X) = \mu$
*   $X \sim U(a, b)$：$E(X) = \frac{a+b}{2}$
*   $X \sim E(\lambda)$：$E(X) = \frac{1}{\lambda}$

---

### **第4.2节 随机变量的方差 (Variance)**

方差衡量的是随机变量取值与其均值的偏离程度，即数据的“波动性”或“分散性”。

#### **1. 定义**

设X是一个随机变量，若 $E[X - E(X)]^2$ 存在，则称此期望为X的**方差**，记作 $D(X)$ 或 $Var(X)$。
$$D(X) = E[X - E(X)]^2$$
方差的算术平方根 $\sigma(X) = \sqrt{D(X)}$ 称为**标准差**。

#### **2. 计算公式**

方差有一个非常实用的计算公式：
$$D(X) = E(X^2) - [E(X)]^2$$
这个公式避免了先计算 $[X - E(X)]^2$ 的复杂过程。

#### **3. 方差的性质**

1.  **非负性**：$D(X) \geq 0$；$D(X) = 0 \Leftrightarrow P(X = E(X)) = 1$
2.  **常数的方差**：$D(c) = 0$
3.  **线性变换**：
    *   $D(cX) = c^2 D(X)$
    *   $D(cX + b) = c^2 D(X)$
4.  **和的方差**：
    *   $D(X + Y) = D(X) + D(Y) + 2Cov(X, Y)$
    *   特别地，若X与Y**相互独立**，则 $D(X + Y) = D(X) + D(Y)$
    *   此性质可推广到n个**相互独立**的随机变量之和。

> **常见错误**：认为 $D(X - Y) = D(X) - D(Y)$ 或 $D(cX) = cD(X)$，这些都是错误的！

#### **4. 常见分布的方差**
*   $X \sim B(n, p)$：$D(X) = np(1-p)$
*   $X \sim P(\lambda)$：$D(X) = \lambda$
*   $X \sim N(\mu, \sigma^2)$：$D(X) = \sigma^2$
*   $X \sim U(a, b)$：$D(X) = \frac{(b-a)^2}{12}$
*   $X \sim E(\lambda)$：$D(X) = \frac{1}{\lambda^2}$

---

### **第4.3节 协方差、相关系数和矩**

这部分研究**多个随机变量之间的关系**。

#### **1. 协方差 (Covariance)**

协方差用于衡量两个随机变量的**联合变化趋势**。

*   **定义**：
    $Cov(X, Y) = E[(X - E(X))(Y - E(Y))]$
*   **计算公式**：
    $Cov(X, Y) = E(XY) - E(X)E(Y)$
*   **性质**：
    *   $Cov(X, Y) = Cov(Y, X)$
    *   $Cov(aX + c, bY + d) = ab Cov(X, Y)$
    *   $Cov(X_1 + X_2, Y) = Cov(X_1, Y) + Cov(X_2, Y)$
    *   $D(X \pm Y) = D(X) + D(Y) \pm 2Cov(X, Y)$
    *   若X, Y独立，则 $Cov(X, Y) = 0$

> **缺点**：协方差受量纲影响，无法比较不同单位的变量间关系的强弱。

#### **2. 相关系数 (Correlation Coefficient)**

相关系数是协方差的“标准化”版本，消除了量纲的影响，用来衡量两个随机变量之间的**线性相关程度**。

*   **定义**：
    $\rho_{XY} = \frac{Cov(X, Y)}{\sqrt{D(X)}\sqrt{D(Y)}}$
*   **性质**：
    1.  $|\rho_{XY}| \leq 1$
    2.  $|\rho_{XY}| = 1 \Leftrightarrow$ X与Y以概率1存在**线性关系**。
    3.  $\rho_{XY} = 0$ 表示X与Y**不相关**（没有线性关系，但可能有其他非线性关系）。
    4.  若X, Y相互独立，则 $\rho_{XY} = 0$（反之不成立！）
    5.  对于二维正态分布，X与Y相互独立 $\Leftrightarrow \rho_{XY} = 0$。

#### **3. 矩 (Moments)**

矩是一类更广泛的数字特征。

*   **k阶原点矩**：$\gamma_k = E(X^k)$
*   **k阶中心矩**：$\mu_k = E([X - E(X)]^k)$
*   **关系**：一阶原点矩是期望 $E(X)$，二阶中心矩是方差 $D(X)$。

---

### **第4.4节 多维正态随机变量**

多维正态分布在理论和应用中都极为重要。

#### **1. n维正态随机变量的定义**

设n维随机向量 $(X_1, X_2, ..., X_n)$ 的联合概率密度由其**协方差矩阵C**决定。其密度函数形式较为复杂，核心在于协方差矩阵。

#### **2. 重要结论**

1.  **线性变换保正态性**：有限个独立正态变量的线性组合仍服从正态分布。
    *   $X \sim N(\mu, \sigma^2)$，则 $aX + b \sim N(a\mu + b, a^2\sigma^2)$
    *   $X \sim N(\mu_1, \sigma_1^2), Y \sim N(\mu_2, \sigma_2^2)$ 且独立，则 $X+Y \sim N(\mu_1+\mu_2, \sigma_1^2 + \sigma_2^2)$
2.  **充要条件**：$(X_1, ..., X_n)$ 服从n维正态分布 $\Leftrightarrow$ 其任意线性组合服从一维正态分布。
3.  **线性组合**：若 $(X_1, ..., X_n)$ 服从n维正态分布，则其任意线性组合构成的新向量也服从多维正态分布。
4.  **独立性与不相关等价**：对于多维正态分布，各分量**相互独立** $\Leftrightarrow$ 各分量**两两不相关**（即协方差矩阵为对角阵）。

---
第四章的内容是概率论的核心部分，从单一变量的期望和方差，扩展到多变量间的协方差和相关系数，最后介绍了具有优良性质的多维正态分布。掌握这些数字特征，为我们进行统计推断、回归分析等后续学习打下了坚实的基础。